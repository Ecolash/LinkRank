# Cross-Project Results — LinkRank (RQ4)

This document summarizes and interprets the cross-project (cross-language) evaluation for our LinkRank family of models. It collects the key numbers and findings for RQ4, highlights comparisons to baseline models, and gives guidance for reproducing and extending the analysis.

## Overview

Goal: evaluate how well LinkRank and LinkRank-C2I transfer when trained on one language (or language group) and tested on others. We ran experiments for three training regimes: Java, C++, and a mixed Rust+Go training split. Results are reported under three selection regimes: Known-K (oracle), Unknown-K (ABS), and Unknown-K (REL).

Datasets: 10 repositories used in the study (see paper/`Summary of Work Done.tex` Table~\ref{stats}).

## Key numerical summary

The primary observations from the cross-project experiments are:

- LinkRank: moderate average drop in F1 under cross-project transfer. Typical numbers observed when training on Rust+Go: ~5% drop under ABS and ~7% drop under REL compared to in-domain performance.
- LinkRank-C2I: larger degradation in some regimes; for Rust+Go training we observed roughly a 10% drop under Known-K and about a 6% drop under ABS in the reported experiments.
- Despite the drops, both models remain competitive and generally outperform baseline methods even after transfer.

### Baseline average degradation (reported in experiments)

The baseline models exhibit heterogeneous transfer robustness. Averaged across test datasets, the reported approximate performance drops were:

| Baseline | Rust+Go ↓ | C++ ↓ | Java ↓ |
|---|---:|---:|---:|
| FRLink | 19.24% | ~18% | ~18% |
| DeepLink | 14.27% | ~8% | 13.51% |
| HybridLinker | 10.9% | 13.11% | ~8% |
| EALink | ~4% | 16.4% | 5.47% |

Notes: these numbers were reported as average drops over the held-out test datasets in the cross-project study. They highlight that (a) FRLink is the most sensitive to language/domain shift, (b) EALink is relatively stable in some settings, and (c) the other baselines show mixed behavior depending on the training language.

## Interpretation and takeaways

- LinkRank's learning-to-rank formulation (LambdaMART on per-issue groups) yields comparatively robust transfer: the model relies on lexical and retrieval signals that generalize reasonably across languages and repositories.
- LinkRank-C2I's bidirectional gating can be helpful in-domain but appears more sensitive to domain shift in some settings (particularly Known-K): the commit→issue shortlist may be less reliable when commit/issue styles change across languages.
- Baselines that rely on brittle heuristics or language-specific features (FRLink, DeepLink) lose more performance under cross-project transfer.
- Practical implication: for deployment across languages, training on a more diverse multilingual corpus or using domain adaptation techniques (few-shot fine-tuning, feature normalization across corpora) will help reduce transfer loss.

## Recommended next steps

1. Data-level: include a mixed-language training set or perform targeted fine-tuning on a small sample of the target language to reduce the drop.
2. Feature-level: add language-agnostic tokens (e.g., function names, file-path signals) and stronger normalization for commit/message lengths.
3. Model-level: explore shallow adaptation strategies (calibrate thresholds per target project, or train a small calibration layer on top of LambdaMART outputs).
4. Evaluation: compute per-repository confidence intervals and error bars for the cross-project numbers to distinguish genuine degradation from sampling noise.

## Files in this folder

- `LinkRank/` — model-specific results, plots and tables for LinkRank cross-project runs.
- `Baselines/` — baseline model results used for comparison in the RQ4 analysis.
- `README.md` — (this file) summary and instructions.

If your results folder contains PNGs or CSVs (plots and raw tables), open them alongside this README to inspect per-dataset behavior.

## Reproduce / quick checks

1. Locate the experiment outputs in the sibling directories above (LinkRank/ and Baselines/). Look for CSV files with per-dataset F1 scores and plotting artifacts (PNG/PDF).
2. To reproduce the summarized drop numbers, compute the mean difference between in-domain F1 and cross-project F1 for each target dataset and average over datasets.
---
*Generated by the LinkRank analysis workflow — place the experiment CSVs and images in the `LinkRank/` or `Baselines/` subfolders to allow automated plotting and reproduction.*


